\chapter{Introduction: What Makes Hardware Verification Complex}

Modern hardware designs are extremely complex.
Such a statement is almost like a cliche in these days, but even today it is still a big challenge.
A single-core processor and a memory system were enough to run computer programs in early days.
Computer architecture has become much more complicated, equipped with multicores, various accelerators (\eg{} GPUs, NPUs, etc.), hardware security modules (HSMs), hierarchical memory subsystems, and so on.
Such hardware components form a large, complex hardware circuit often called a system on a chip (SoC).

Concurrency is the major challenge in hardware design.
In terms of maximizing utility of hardware components, concurrent execution has been very natural in hardware.
For instace, in a high level, processing units and a memory subsystem run concurrently; \eg{} a processor core may execute other instructions after making a memory request, while the memory handles it.
The processor core may be pipelined, which is very common in these days, implying that (computer-architecture) instructions are executed concurrently as well.
The memory subsystem may be composed of several caches and the main memory, and several requests from different cores may be handled concurrently in a distributed way throughout the caches and the memory.
Concurrency does not mean complete parallelism.
In order for the correct implementation, some orders among execution units should be maintained, and managing this constraint is indeed a great challenge.

As hardware has become large with extreme concurrency, verification of the hardware has become much more challenging as well.
Industry has most commonly used testing and bounded model checking (BMC)~\cite{BMC} for quality assurance of hardware circuits, but these techniques are not sound and may miss subtle bugs.
Due to such a shortcoming, both industry and academia have developed so-called \emph{formal-verification} techniques.
At least in hardware verification, there are two mainstream approaches to formally verifying hardware: model checking and theorem proving.

Model checking~\cite{ModelChecking1, ModelChecking2} basically tries to verify a hardware module by exploring all reachable states, checking whether the desired properties hold with those states.
The biggest benefit of model checking is that it is automated.
That being said, state-space explosion is a typical problem in model checkers; it often requires too much time and (memory) space to explore all reachable states.

Theorem proving, also called deductive verification, provides a mechanism to generate proofs based on a certain set of logics.
The theorems are proven either manually by a user (called interactive theorem proving), automatically with various solvers (called automated theorem proving), or in a combined way (both interactive and automated).
The biggest benefit of theorem proving is that once a theorem is proven, it can be used repeatedly for bigger proofs.
Theorem provers usually provide rich logics, and thus allow a user to state theorems general enough to be reused.
That being said, a theorem prover usually requires the user to understand the target computer system deeply enough to state theorems and proofs in detail.

\subsubsection{Problem statement}

Among various hardware-verification challenges, this dissertation specifically deals with cache-coherence protocols with the Coq proof assistant~\cite{Coq} (an interactive theorem prover).

Cache-coherence protocols have been regarded as one of the greatest correctness challenges of the hardware world.
As per typical distributed-protocol verification, the main challenge comes while dealing with interleavings among transactions.
In cache-coherence protocols, a request from a processor may require multiple caches to change their local states, and we call a sequence of such state transitions a \emph{transaction}.
Thus it is natural that different cache objects concurrently handle different requests, which makes the transactions \emph{interleaved}.
Interleavings make the verification much more difficult, by significantly increasing the number of states to be explored.

Research on model checking has produced sound techniques to overcome state-space explosion by interleavings.
One of the core techniques is to establish \emph{noninterference} lemmas~\cite{McMillan:1999,McMillan:2001,Chou:2004,flow:Talupur:2008,flow:OLeary:2009,flow:Sethi:2014}, which ensure that concurrent executions of transactions are not spuriously interleaved.
In addition to this resolution, model checkers have supported \emph{parameterization} to verify cache-coherence protocols with unknown numbers of cores~\cite{Zhang:2010,Zhang:2014,Banks:2017}.
In order to cover hierarchical protocols, recent approaches~\cite{Chen:2008,Chen:2010,Opeoluwa:2016,Opeoluwa:2017,Oswald:2020} also employed \emph{modularity} to verify the protocol per hierarchy level.

While the same concern exists in theorem proving (by requiring additional proof effort to deal with interlevings), it also has employed similar techniques.
Parameterization is a natural fit for theorem proving, thanks to rich logics, making it possible to prove a cache-coherence protocol with an arbitrary tree shape given as a parameter~\cite{Murali:2015}.
Modularity has been harnessed as well, \eg{} with assume-guarantee reasoning~\cite{McMillan:2016}.

The established style of noninterference reasoning, though, has an inherent weakness: it requires explicit lemma statements per-protocol.
The lemmas are often described explicitly by the user (guided by intuition) or discovered automatically (typically guided by counterexamples).
Another issue is that even though these lemmas are stated and proven, it is hard to be sure if the lemmas are enough to resolve all possible interleavings in the system.

Use of modularity has also imposed significant restrictions in protocol design, in order to obtain clear module boundaries among caches.
A concrete limitation of modular design and verification comes when trying to verify cache-coherence protocols that are not \emph{inclusive}.
Noninclusive caches have been in common industrial use for a decade: AMD Opteron servers are known to use exclusive caches~\cite{Irazoqui:2016}, and Intel Skylake-X processors use noninclusive caches~\cite{intel-non-inclusive,Zhao:2010,Yan:2019}.

We would like to avoid all of these weaknesses via a new approach doing mechanized proof in an expressive logic, stating and proving once-and-for-all a general property and using it multiple times.
Particularly, in proving cache-coherence protocols, we want to formalize \emph{the most-general form of noninterference}, find minimal and abstract conditions \emph{that are not coupled to any specific protocols} but still imply noninterference, state the implication as a theorem, prove it once, and use it repeatedly for various cache-coherence protocols.

The abstract conditions that imply the noninterference can be found from the intuitions that protocol designers already have.
In order to describe a case where the protocol deals with two different requests passing through the same cache, for instance, a designer often says ``this cache acts like a \emph{serialization point} so the two transactions can safely \emph{interleave} with each other.''
While the notions of interleaving and serialization are already pervasive, we have found no past work trying to find a minimal set of conditions that ensure transactions are always serialized, formalizing them in a reusable framework.

We go one step further: rather than requiring the user to prove the conditions per-protocol, we establish a framework that guides the user to design protocols that automatically satisfy the conditions, without worrying about possible complex interleavings among different transactions.
The framework does not require the user to write any noninterference properties or to design the protocol in a modular manner.

In this paper, we introduce the \hemiola{} framework, embedded in the Coq proof assistant, which eliminates the burden of designing and proving correct interleavings in cache-coherence protocols.
Specifically, \hemiola{} formalizes a set of message-passing distributed protocols with tree hierarchies and particular topologies of channels between nodes, with associated locks.
\hemiola{} exposes a domain-specific language of protocols, such that any expressible protocol is guaranteed to exhibit ``general'' noninterference, which will be called \emph{serializability} and formalized later in the dissertation.

Furthermore, \hemiola{} provides a novel invariant proof method that only requires consideration of execution histories that do not interleave memory accesses -- so that almost all formal reasoning about concurrency is already done by the framework, not proofs of individual protocols.

To demonstrate usability of our framework, we provide complete correctness proofs for hierarchical cache-coherence protocols, in both inclusive and noninclusive variants, as case studies.
We also demonstrate that the case-study protocols are indeed hardware-synthesizable, by using a compilation/synthesis toolchain in \hemiola{}.

%% All the code and proofs of the framework and the case studies are available in the supplementary material we provided for the paper submission.

\subsubsection{Contributions}

To sum up, the contribution of this dissertation includes the following:
\begin{itemize}
\item We discover a minimal set of topology and lock conditions that ensures \emph{serializability}, extracted from usual cache-coherence protocol designs.
  We then identify a domain-specific protocol language, where every protocol defined in this language ensures serializability by construction, backed up with mechanized Coq proof (\autoref{sec-hemiola-dsl}).
  Lastly, we formalize how serializability helps prove correctness (trace refinement), by using the novel notion of \emph{predicate messages} in distributed protocols.
\item Using the protocol language, we provide the complete correctness proofs of three hierarchical cache-coherence protocols: inclusive and noninclusive MSI protocols and a noninclusive MESI protocol (\autoref{sec-case-study}).
  Ours are the first complete mechanized proofs for various hierarchical ``noninclusive'' cache-coherence protocols, and as a bonus they share a large segment of reusable proofs (including with the inclusive variant).
\item We provide a toolchain to compile \hemiola{} protocols to RTL implementations and to synthesize them to run on FPGAs.
  We also evaluate our case-study protocols, in order to demonstrate that they indeed run on hardware with reasonable performance.
\end{itemize}

\subsubsection{Outline}

Before introducing our proposed techniques to verify cache-coherence protocols, we will provide some background to understand conventional protocol designs and typical correctness challenges in \autoref{sec-background}.
After the background chapter, this dissertation is composed of three parts.

The first part presents our underlying theory to reason about cache-coherence protocols.
\autoref{sec-trs} introduces protocol transition systems as underlying state-transition systems to describe behaviors of cache-coherent memory subsystems.
On top of a protocol transition system, we provide the formal definition of serializability in \autoref{sec-sz-def}.
This chapter also demonstrates how serializability helps state and prove invariants of a given system.

The second part introduces \hemiola{}, a framework embedded in Coq to design and verify cache-coherence protocols more structurally, by using the serializability guarantee.
\autoref{sec-hemiola-dsl} first presents the \hemiola{} domain-specific language for easier protocol design.
\autoref{sec-sz-guarantee-hemiola} describes the biggest contribution of the framework, a general once-and-for-all property claiming that any protocol defined with the DSL obtains the serializability property for free.
We then provide the first series of related work in \autoref{sec-related-work-i}, focusing on how previous approaches dealt with interleavings among transactions.

The last part provides our case studies, design and verification of hierarchical cache-coherence protocols.
We present the designs and complete correctness proofs of hierarchical MSI and MESI protocols on top of the \hemiola{} framework in \autoref{sec-case-study}.
\autoref{sec-comp-syn} presents the protocol compiler, which compiles a protocol described in \hemiola{} to a corresponding RTL implementation, and discusses the correctness of the compiler.
We then provide the other series of related work in \autoref{sec-related-work-ii}, exploring some previous approaches to verifying cache-coherence protocols.

\chapter{Background}
\label{sec-background}

Before introducing our proposed method to design and prove cache-coherence protocols more structurally, in this chapter we provide some background to understand conventional cache-coherence protocol designs and typical correctness challenges.
We first introduce a very simple cache-coherence protocol to explain the purpose of the protocol, basic design, and a number of nontrivial corner cases that require careful handling.
After that, we explore some design space of cache-coherence protocols for better understanding of the case-study protocols we will provide in \autoref{sec-case-study}.

\section{Cache-Coherence Protocols In a Nutshell}
\label{sec-nutshell}

In this section, we provide a simple yet motivating example to explain the typical challenges.
For simplicity, throughout the section, we will consider a protocol protecting only \emph{a single memory location}.
We will see it is still nontrivial to design and verify a correct protocol.

The overall goal of cache coherence is, as the name suggests, to preserve coherence among multiple candidate values in a memory subsystem.
In other words, if the system is coherent, then it should behave like an atomic memory.
This behavior inclusion is formally defined as \emph{refinement}, which will be introduced in \autoref{sec-semantics}.
The correctness of a cache-coherence protocol is sometimes shown by proving representative invariants, not talking about the relation to the desired spec.
The single-writer/multiple-reader (SWMR) invariant and the data-value invariant~\cite{ccbook:2020} are classic choices; in proving the refinement of a protocol, these invariants (or similar ones) must be proven.

\begin{figure}[t]
  \centering
  \begin{tikzpicture}
    \pic at (0, 0) {skeleton-pcce12={$P(S, v, S_{\tuple{1, 2}})$}{$C_1(S, v)$}{$C_2(S, v)$}};
    \pic at (0, 0) {skeleton-midx-e1};
    \pic at (0, 0) {skeleton-midx-e2};
    \pic at (0, 0) {skeleton-midx-pc1};
    \pic at (0, 0) {skeleton-midx-pc2};
    \node at (3.0, -1) {$\sqsubseteq$};
    \pic at (4.5, -0.7) {spec};
  \end{tikzpicture}
  \caption{A simple MSI directory protocol and its spec}
  \label{fig-motive-1}
\end{figure}

\autoref{fig-motive-1} shows caches and communication channels for a simple directory-based MSI protocol (LHS of $\sqsubseteq$).
Since we deal with only a single memory location, the specification (RHS of $\sqsubseteq$) is a single-value ($v$) container with atomic read and write.
There are three caches ($P$, $C_1$, and $C_2$) in the implementation, and each of them has its own status (M, S, or I) and data ($v$).
The status of a cache represents a permission on its local replica.
In this MSI protocol, an object can read/write the data with the M (``modified'') status, only read with S (``shared''), and cannot read/write with I (``invalid'').
The parent $P$ additionally has a data structure called a \emph{directory} to track the statuses of the children.
For example, a directory might be $S_{\tuple{1, 2}}$, meaning that both $C_1$ and $C_2$ have S status, in some logical snapshot of state.

Objects communicate through ordered channels, shown as $(\rightarrowtail)$ in the figure, where each of it has a unique index (shown as a natural number in the figure).
$C_1$ and $C_2$ have channels to receive and respond to external requests (from processor cores).
There are three types of channels between a parent and a child: a single channel for parent-to-child messages and two channels for child-to-parent requests and responses, respectively.
It is natural to wonder why two separate channels are required from a child to a parent; we will see the reason very soon.
Note that channels are depicted in a logical way; the actual hardware implementation may use different hardware components (\eg{} finite-capacity FIFOs or buses) that can simulate ordered channels.

\begin{figure}[t]
  \centering
  \begin{tikzpicture}
    \pic at (0, 0) {skeleton-pcce12={$P(S, v, S_{\tuple{1, 2}})$}{$C_1(S, v)$}{$C_2(S, v)$}};
    % C_1 external
    \pic at (0, 0) {skeleton-midx-e1};
    \node[label={[label distance=-6pt,myred]left:{\rdrecmsgsm{a}{rqWr}}},color=myred] at (-1.8, -2.35) {$\bullet$};
    % C_2 external
    \pic at (0, 0) {skeleton-midx-e2};
    \node[label={[label distance=-6pt,myblue]left:{\blrecmsgsm{e}{rqWr}}},color=myblue] at (1.6, -2.35) {$\bullet$};
    % Between P and C_1
    \pic at (0, 0) {skeleton-midx-pc1};
    \node[label={[label distance=-6pt,myred]left:{\rdrecmsgsm{b}{rqM}}},color=myred] at (-1.1, -0.8) {$\bullet$};
    % Between P and C_2
    \pic at (0, 0) {skeleton-midx-pc2};
    \node[label={[label distance=-9pt,myred]below right:{\rdrecmsgsm{d}{rsI}}},color=myred] at (0.9, -0.8) {$\bullet$};
    \node[label={[label distance=-9pt,myred]above right:{\rdrecmsgsm{c}{rqI}}},color=myred] at (1.1, -0.8) {$\bullet$};
    \node[label={[label distance=-9pt,myblue]below left:{\blrecmsgsm{f}{rqM}}},color=myblue] at (0.7, -0.8) {$\bullet$};

    % Curves
    \draw [->,color=myred] (-2.95, -2.05) to[out=90,in=-110] node {\rdcircf{1}} (-2.35, -1.05);
    \draw [->,color=myred] (-2.25, -0.5) to[out=70,in=110,distance=2cm] node {\rdcircf{3}} (1.7, -0.2);
    \draw [->,color=myred] (2.2, -0.7) to[out=-40,in=-40,distance=1.2cm] node {\rdcircf{4}} (2.0, -1.1);
    \draw [->,color=myblue] (0.7, -2.0) to[out=90,in=-45] node {\blcircf{2}} (0.3, -1.4);

  \end{tikzpicture}
  \caption{Rule-execution cases in the simple MSI protocol}
  \label{fig-motive-2}
\end{figure}

\autoref{fig-motive-2} depicts some example state-transition cases depending on statuses of the caches.
All the caches run concurrently, repeatedly executing \emph{rules} that define local state transitions.
A rule may take some messages from input channels, perform a state transition, and put messages in output channels.
A rule may also have a precondition, blocking use of that rule when the precondition does not hold.
$\rdcircf{1}$ shows the case where a child $C_1$ takes an external request (\rdrecmsgsm{a}{rqWr}) to write data, but it does not have M status and thus further requests to the parent (\rdrecmsgsm{b}{rqM}) to get the permission.
At this moment, in many cache-coherence protocol designs, $C_1$ changes its status to a \emph{transient state} SM to record its current status (S) and the next expected status (M).
This transient state also functions as a \emph{lock} not to allow any further external requests for the value.

Due to the concurrent execution of the caches, we might have another rule executed at the same time.
$\blcircf{2}$ is executed concurrently with $\rdcircf{1}$, where $C_2$ also takes an external request (\blrecmsgsm{e}{rqWr}) with the same purpose, thus requests \blrecmsgsm{f}{rqM} to the parent as well.
Since $\rdcircf{1}$ and $\blcircf{2}$ happened at the same time, now the parent $P$ needs to decide which request to deal with.
Suppose that it decided to handle \rdrecmsgsm{b}{rqM} first.

$\rdcircf{3}$ presents the next execution by $P$, taking the input message \rdrecmsgsm{b}{rqM} and making an \emph{invalidation} request (\rdrecmsgsm{c}{rqI}) to the other child $C_2$ to change its status to I.
This invalidation request is required, since when a child has M the others should not be able to read/write the data.
The parent, at this moment, changes its directory status to a transient state (named $S^D$ in some textbooks) to disallow any other requests from the children.
For instance, the parent should not handle \blrecmsgsm{f}{rqM}, since otherwise it will handle two ``\msgsf{rqM}''s simultaneously, which might lead to an incoherent state -- two M statuses in the caches.

Lastly, $\rdcircf{4}$ shows the case that $C_2$ handles the invalidation request (\rdrecmsgsm{c}{rqI}).
A number of corner cases should be handled carefully in this case:
\begin{itemize}[leftmargin=*]
\item Since $C_2$ requested \blrecmsgsm{f}{rqM}, it has a transient state SM when \rdrecmsgsm{c}{rqI} arrives. It should still be able to handle this invalidation request even in the transient state (while not allowing any external requests). In this case $C_2$ accepts \rdrecmsgsm{c}{rqI} and changes its transient state to IM. We see that transient states should be fine-grained enough to distinguish which requests to handle.
\item Due to the existence of \blrecmsgsm{f}{rqM}, if we had a single channel from a child to a parent, a deadlock would occur. $P$ cannot take \blrecmsgsm{f}{rqM} since it has been locked (in a transient state) after making an invalidation request. It cannot take \rdrecmsgsm{d}{rsI} as well since the response is not the first one of the ordered channel. This case shows the necessity of having multiple channels between a child and a parent.
\end{itemize}

A so-called three-channel system has been widely used and regarded as a good choice to make the design correct and live~\cite{Murali:2015,thesis:Murali:2016}.
While there are other possible correct topology and network settings, the cases shown in \autoref{fig-motive-2} at least demonstrate that it is nontrivial to construct one of them.

\begin{figure}[t]
  \centering
  \begin{tikzpicture}
    \pic at (0, 0) {skeleton-pcce12={$P(S, v, S_{\tuple{1, 2}})$}{$C_1(S, v)$}{$C_2(S, v)$}};
    % C_1 external
    \pic at (0, 0) {skeleton-midx-e1};
    \node[label={[label distance=-6pt,myred]right:{\rdrecmsgsm{i}{rsWr}}},color=myred] at (-1.6, -2.35) {$\bullet$};
    % C_2 external
    \pic at (0, 0) {skeleton-midx-e2};
    % Between P and C_1
    \pic at (0, 0) {skeleton-midx-pc1};
    \node[label={[label distance=-12pt,myred]below right:{\rdrecmsgsm{g}{rsM}}},color=myred] at (-0.9, -1.0) {$\bullet$};
    \node[color=myblue] at (-0.5, -0.6) {$\bullet$};
    \node at (-1.5, -0.3) {\blrecmsgsm{h}{rqI}};
    \draw [densely dashed,color=myblue,line width=0.3pt] (-0.5, -0.6) to[out=170,in=-10] (-1.1, -0.3);

    \node[color=myblue] at (-0.8, -0.7) {$\bullet$};
    \node at (-2.5, -0.5) {\blrecmsgsm{j}{rsI}};
    \draw [densely dashed,color=myblue,line width=0.3pt] (-0.8, -0.7) to[out=190,in=-10] (-2.05, -0.5);

    % Between P and C_2
    \pic at (0, 0) {skeleton-midx-pc2};
    \node[label={[label distance=-9pt,myred]above right:{\rdrecmsgsm{d}{rsI}}},color=myred] at (0.9, -0.8) {$\bullet$};
    \node[color=myblue] at (0.7, -0.8) {$\bullet$};
    \node at (2.0, 0.2) {\blrecmsgsm{f}{rqM}};
    \draw [densely dashed,color=myblue,line width=0.3pt] (0.7, -0.8) to[out=70,in=210] (1.5, 0.1);

    % Curves
    \draw [->,color=myred] (1.05, -0.5) to[out=180,in=60] node {\rdcircf{5}} (-0.6, -1.1);
    \draw [->,color=myred] (-0.6, -1.5) to[out=-90,in=80] node {\rdcircf{7}} (-0.8, -2.2);
    \draw [->,color=myblue] (2.0, 0.4) to[out=130,in=80,distance=1.5cm] node {\blcircf{6}} (-1.5, -0.1);
    \draw [->,color=myblue] (-1.5, -0.5) to[out=245,in=-115,distance=0.9cm] node {\blcircf{8}} (-2.5, -0.8);
    \draw [->,color=myblue] (-2.5, -0.3) to[out=90,in=-100] (-2.0, 0.7);
    \node[color=myblue] at (-2.0, 1.1) {$\vdots$};

  \end{tikzpicture}
  \caption{Rule-execution cases, continued}
  \label{fig-motive-3}
\end{figure}

\autoref{fig-motive-3} presents some additional rule-execution cases right after $\rdcircf{4}$ in \autoref{fig-motive-2}.
After $\rdcircf{4}$ responds with \rdrecmsgsm{d}{rsI}, now $P$ can take it to respond back to $C_1$.
$\rdcircf{5}$ presents this step, taking \rdrecmsgsm{d}{rsI} and responding back to $C_1$ with \rdrecmsgsm{g}{rsM}.
At this moment $P$ changes its status to $P(I, ?, M_{(1)})$ (from the transient state $S^D$) to record that it no longer has the coherent value and just granted $C_1$ the M status.
Since the transient state also functions like a lock, this status change can be also regarded as a lock release, so that $P$ can handle some other requests.

$\blcircf{6}$ is the next transition step by $P$ to accept a new request.
\blrecmsgsm{f}{rqM} has been waiting for the transient state of $P$ to be released, and $\blcircf{6}$ finally takes it and sends an invalidation request to $C_1$.

Looking at the message channel $5$, the one for parent-to-child messages (from $P$ to $C_1$), there are two messages, \rdrecmsgsm{g}{rsM} and \blrecmsgsm{h}{rqI}, residing in the channel.
A single parent-to-child channel affects the correctness of the protocol in this case.
If we had two separated downward channels, one for requests and the other for responses, $C_1$ could handle \blrecmsgsm{h}{rqI} first, changes its status to I, handles \rdrecmsgsm{g}{rsM} later, and changes its status to M again.
Since it sent the invalidation response to the parent, eventually $C_2$ will also get the M status, which leads to an incoherent state.
Therefore, it is crucial to have a single parent-to-child channel so that \rdrecmsgsm{g}{rsM} \emph{blocks} \blrecmsgsm{h}{rqI} to be handled first.
After $C_1$ takes \rdrecmsgsm{g}{rsM} and responds back to the processor core with \rdrecmsgsm{i}{rsWr}, presented as $\rdcircf{7}$, it can subsequently take the invalidation request \blrecmsgsm{h}{rqI} and responds with \blrecmsgsm{j}{rsI}, as shown in $\blcircf{8}$.

As examined in \autoref{fig-motive-2} and \autoref{fig-motive-3}, in a cache-coherence protocol, transient states and network channels play a crucial role in making \emph{interleavings} correct.
Regarding the sequence of rule executions (in red) $[\rdcircf{1}; \rdcircf{3}; \rdcircf{4}]$ as an execution flow in \autoref{fig-motive-2} -- we will call it a \emph{transaction} in later sections -- to handle the original external request \rdrecmsgsm{a}{rqWr}, we see that no further executions could happen after $\blcircf{2}$, which is for the other request \blrecmsgsm{e}{rqWr}.
As explained above case-by-case, proper transient states and network channels block \blrecmsgsm{f}{rqM} from further processing.
Similarly, in \autoref{fig-motive-3}, the execution flow $[\rdcircf{5}; \rdcircf{3}]$ was not spuriously affected by $\blcircf{6}$ or $\blcircf{8}$, thanks to the correct channel setting.

If proper locking (by transient states) and topology are crucial for designing a correct protocol, can we craft a domain-specific language where only conformant protocols are expressible?
That is exactly what \hemiola{} provides: it provides \emph{rule templates}, explained in \autoref{sec-rule-templates}, that employ proven-safe topologies and network structures and automatically set associated locks, so designers can design protocols without worrying about corner cases due to interleavings.

\section{Design Space of Cache-Coherence Protocols}
\label{sec-design-space}

In \autoref{sec-nutshell}, we examined a very specific protocol design.
There are different ways to design a cache-coherence protocol, and each design decision affects performance and correctness of the protocol.
In this section, we explore major design space of a cache-coherence protocol.

\subsubsection{Snooping vs. Directory}

A typical classification of a cache-coherence protocol is whether the protocol employs a designated data structure to keep track of the statuses of child caches.
There are two representative classes: \emph{snooping} and \emph{directory} protocols.

A snooping protocol does not use any data structure to monitor the statuses of child caches.
Instead, when the parent communicates with child caches, it \emph{broadcasts} a message to the children.
Each child cache decides whether to respond to the message from the parent or just to ignore it, depending on its status.
For example, suppose the scenario, already mentioned in \autoref{sec-nutshell}, that the parent wants to an invalidation request to downgrade the status of the child who has the M status to I.
Since the parent cache does not have any information which child currently has the M status, it has no choice but to broadcast the invalidation-request message to all of the children.
The child cache with the M status will respond to the parent after invalidating its status, while the other caches will just ignore the request.
In actual hardware implementations, snooping is usually implemented with an ordered wire bus shared by the parent and the children.

A directory protocol, as its name says, uses the directory structure to record which child has which status.
The example presented in \autoref{sec-nutshell} is a directory protocol; we see that the parent $P$ additionally has a directory data structure.
Considering the same invalidation scenario, because the parent knows which child has the M status exactly, it can make the invalidation request just to that child.

A snooping protocol is known to be easier to implement than a directory protocol, since the ordered bus enforces less interleavings among transactions.
That being said, the snooping protocol is also known not to be scalable, since the cost of broadcasting increases significantly when more child caches are involved.
A directory protocol, on the other hand, is known to be scalable and especially matches well hierarchy layers, but the design and verification are relatively more difficult than the snooping protocol.

This dissertation only deals with directory protocols due to their inherent difficulties.
We will build a framework that can ease the burden of designing and verifying directory protocols, and demonstrate the practicality of the framework with various hierarchical directory protocols as case studies.

\subsubsection{Write-update vs. Write-invalidate}

A cache-coherence protocol can be also classified by the patterns of memory writes.
Amony various memory-write patterns, we classify the protocol by when each legal write to a cache is propagated to the other caches.
Then we have two classes: \emph{write-update} and \emph{write-invalidate} protocols.

In a write-update protocol, if a cache wants to write to a line, it requests to update the lines in the other caches first and updates its line after the updates by the others.
This protocol is beneficial when a cache writes to a line and the other caches wants to read it right after the write.

In a write-invalidate protocol, if a cache wants to write to a line, it first requests to invalidate the lines in the other caches so that they cannot read or write the line.
Once all the other caches are invalidated, it can write a new value.
The write-invalidate policy is used more generally in practical cache-coherence protocols, since the write-update policy requires more bandwidth (to carry the up-to-date written value) and is not efficient when some other caches actually do not need to read the up-to-date value anymore.

\subsubsection{Write-through vs. Write-back}

Another criterion about memory writes is when a write to a cache is applied to the main memory; it provides two classes as well: \emph{write-through} and \emph{write-back} protocols.

In a write-through protocol, when a cache writes to a line (already with enough permission), it further requests a write to the main memory as well.
This protocol is easier to design, since the up-to-date value can be always found in the main memory.

On the other hand, in a write-back protocol, a cache does not requests to the main memory to send the value when writing to a line.
The write-back policy is harder to implement than the write-through protocol, since the protocol should provide a way to find the up-to-date -- possibly dirty -- value among all the caches, \eg{} by managing directories that point the cache that has the value.
That being said, almost all practical cache-coherence protocols use the write-back policy to avoid unnecessary writeback to the main memory.

All the case-study protocols in this dissertation, which will be introduced in \autoref{sec-case-study}, follow the write-invalidate and write-back policies.

\subsubsection{Inclusive vs. Noninclusive vs. Exclusive}

The protocol we introduced in \autoref{sec-nutshell} is a flat (\ie{} nonhierarchical) protocol in that child caches ($C_1$ and $C_2$) communicates directly with the parent $P$ as the main memory with a directory.
In other words, there are no intermediate caches between the lowest-level caches (called the L1 caches) and the main memory.
There is an essential tradeoff between caches and the memory: caches have much faster latency than the memory but lower hit rate.
In order to mitigate the hit rate of a cache, hierarchical cache-coherence protocols have been developed and used.
In a hierarchical protocol, some of the L1 caches are clustered to have an L2 cache as the parent, and so on.
A higher-level cache usually has a lower latency than lower-level caches but has a better hit rate due to its increased size.

Hierarchical cache-coherence protocols can be classified by so-called the \emph{cache-inclusion policy}.
Especially when designing a protocol with directories, it is practically easier to design caches to follow the requirement that every line in the value cache has its corresponding directory entry in the directory cache, and vice versa.
It is indeed easier to design with such a policy, since it allows to combine an information cache (\ie{} the one containing line statuses, etc.) and a directory cache into a single cache, which is more space-efficient by not having to store tags in each cache.
This policy, in other words, means that whenever a line is in a child cache there is a corresponding line in the parent cache as well.
We therefore call it \emph{inclusive}, meaning that the parent cache includes the lines in child caches.

Inclusive caches have a benefit that certain requests from the parent can be responded immediately just by searching the directory status.
For example, when a cache gets an invalidation request from the parent and the line entry does not exist in the directory, it does not need to forward the request to the children for the complete invalidation, since by the inclusion policy absense of the directory status already implies that the line does not exists in any of the child caches.
That said, inclusive caches have a drawback that some value entries are wasted; for example, when a directory status is M, meaning that a child has the possibly dirty value, the parent does not need to hold any stale value inside the value cache.

A \emph{noninclusive} cache -- also called a noninclusive nonexclusive (NINE) cache -- does not require such inclusion.
Since it does not require the inclusion, it has less chance to waste value lines.
That said, the biggest drawback comes when a directory-status entry does not exist in the directory cache; the absense of the directory status does not imply that child caches do not have the line.
The same issue occurs in a snooping protocol as well; \eg{} a cache should always snoop all the child caches in order to handle an invalidation request from the parent.

In order to resolve the addressed issues, a number of practical cache-coherence protocols are known to manage noninclusive (value) caches and inclusive directories, called NCID~\cite{Zhao:2010}.
Intel Skylake-X processors are known to use noninclusive caches~\cite{intel-non-inclusive,Yan:2019}.
By having noninclusive value caches we have less chance to waste value lines, and by having inclusive directories we do not always need to visit child caches for invalidation.

On the contrary to the inclusive cache, an \emph{exclusive} cache intends to maximize the utility between child caches and the parent one, by requiring that the lines of each child and the parent are disjoint to each other.
AMD Opteron servers are known to use exclusive caches~\cite{Irazoqui:2016}.
A similar optimization to NCID is possible in exclusive caches, by managing directories as inclusive while the value caches are exclusive.
